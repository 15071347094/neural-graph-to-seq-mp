# Neural Graph to Sequence Model

This repository contains the code for our paper [A Graph-to-Sequence Model for AMR-to-Text Generation](https://arxiv.org/pdf/1805.02473.pdf) in ACL 2018

The code is developed under TensorFlow 1.4.1 

## Data precrocessing
Our current data loading [code](./src_g2s/G2S_data_stream.py) requires simplified AMR graphs where variable tags, sense tags and quotes are removed. For example, the following AMR
```
(d / describe-01 :ARG0 (p / person :name (n / name :op1 "Ryan")) :ARG1 p :ARG2 genius)
```
can be simplified as
```
describe :arg0 ( person :name ( name :op1 ryan )  )  :arg1 person :arg2 genius
```
Our simplifier can be downloaded via [here](https://www.cs.rochester.edu/~lsong10/downloads/amr_simplifier.tgz). It is generated by modifying the [NeuralAMR](https://github.com/sinantie/NeuralAmr) code.

Another alternative is to write your own data loading code according to the format of your own AMR data. 

## Training

First, modify the PYTHONPATH within [train_g2s.sh](./train_g2s.sh) (for our graph-to-string model) or [train_s2s.sh](./train_s2s.sh) (for baseline). <br>
Second, modify config_g2s.json or config_s2s.json. You should pay attention to the field "suffix", which is an identifier of the model being trained and saved. We usually use the experiment setting, such as "bch20_lr1e3_l21e3", as the identifier. <br>
Finally, execute the corresponding script file, such as "./train_g2s.sh".

### Using large-scale automatic AMRs

In this setting, we follow [Konstas et al., (2017)](https://arxiv.org/abs/1704.08381) to take the large-scale automatic data as the training set, with the original gold data being the finetune set. To train in this way, you need add a new field "finetune_path" in the configuration file and point it to the gold data. Then, point "train_path" to the automatic data. 

## Decoding with a pretained model

Simply execute the corresponding decoding script with one argument being the identifier of the model you want to use.
For instance, you can execute "./decode_g2s.sh bch20_lr1e3_l21e3"

## Cite
If you like our paper, please cite
```
@InProceedings{song-EtAl:acl2018,
  author    = {Song, Linfeng  and  Zhang, Yue  and  Wang, Zhiguo  and  Gildea, Daniel},
  title     = {A Graph-to-Sequence Model for {AMR}-to-Text Generation},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL-18)},
  year      = {2018},
  address   = {Melbourne, Australia},
}
```
